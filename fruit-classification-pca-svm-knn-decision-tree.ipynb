{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRUIT CLASSIFICATION using PCA, SVM, Decision Trees and k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "print(os.listdir(\"/input\"))\n",
    "dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def getYourFruits(fruits, data_type, print_n=False, k_fold=False):\n",
    "    images = []\n",
    "    labels = []\n",
    "    val = ['Training', 'Test']\n",
    "    if not k_fold:\n",
    "        path = \"/input/*/fruits-360/\" + data_type + \"/\"\n",
    "        for i,f in enumerate(fruits):\n",
    "            p = path + f\n",
    "            j=0\n",
    "            for image_path in glob.glob(os.path.join(p, \"*.jpg\")):\n",
    "                image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "                image = cv2.resize(image, (dim, dim))\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                images.append(image)\n",
    "                labels.append(i)\n",
    "                j+=1\n",
    "            if(print_n):\n",
    "                print(\"There are \" , j , \" \" , data_type.upper(), \" images of \" , fruits[i].upper())\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "        return images, labels\n",
    "    else:\n",
    "        for v in val:\n",
    "            path = \"/input/*/fruits-360/\" + v + \"/\"\n",
    "            for i,f in enumerate(fruits):\n",
    "                p = path + f\n",
    "                j=0\n",
    "                for image_path in glob.glob(os.path.join(p, \"*.jpg\")):\n",
    "                    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "                    image = cv2.resize(image, (dim, dim))\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                    images.append(image)\n",
    "                    labels.append(i)\n",
    "                    j+=1\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "        return images, labels\n",
    "    \n",
    "def getAllFruits():\n",
    "    fruits = []\n",
    "    for fruit_path in glob.glob(\"../input/*/fruits-360/Training/*\"):\n",
    "        fruit = fruit_path.split(\"/\")[-1]\n",
    "        fruits.append(fruit)\n",
    "    return fruits\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHOOSE CLASS\n",
    "**Cocos** and **Pineapple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose your Fruits\n",
    "fruits = ['Pineapple' , 'Cocos'] #Binary classification\n",
    "\n",
    "#Get Images and Labels \n",
    "X_t, y_train =  getYourFruits(fruits, 'Training', print_n=True, k_fold=False)\n",
    "X_test, y_test = getYourFruits(fruits, 'Test', print_n=True, k_fold=False)\n",
    "\n",
    "#Get data for k-fold\n",
    "X,y = getYourFruits(fruits, '', print_n=True, k_fold=True)\n",
    "\n",
    "#Scale Data Images\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform([i.flatten() for i in X_t])\n",
    "X_test = scaler.fit_transform([i.flatten() for i in X_test])\n",
    "X = scaler.fit_transform([i.flatten() for i in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is converted in a 100x100 numpy array for each RGB dimension (x3). Then has been flatted in one single vector (Image Features Vector) and then scaled subtracting the mean of the dataset in order to perform classification algorithms.\n",
    "\n",
    "In the image below the last matrix is our **X_train**, with a shape of 30000x980\n",
    "\n",
    "![](https://i.imgur.com/lC1pcrV.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION OF DATA\n",
    "Let's see now how one of our samples appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def plot_image_grid(images, nb_rows, nb_cols, figsize=(15, 15)):\n",
    "    assert len(images) == nb_rows*nb_cols, \"Number of images should be the same as (nb_rows*nb_cols)\"\n",
    "    fig, axs = plt.subplots(nb_rows, nb_cols, figsize=figsize)\n",
    "    \n",
    "    n = 0\n",
    "    for i in range(0, nb_rows):\n",
    "        for j in range(0, nb_cols):\n",
    "            axs[i, j].axis('off')\n",
    "            axs[i, j].imshow(images[n])\n",
    "            n += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruits[y_train[0]])\n",
    "plot_image_grid(X_t[0:100], 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fruits[y_train[490]])\n",
    "plot_image_grid(X_t[490:590], 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def getClassNumber(y):\n",
    "    v =[]\n",
    "    i=0\n",
    "    count = 0\n",
    "    for index in y:\n",
    "        if(index == i):\n",
    "            count +=1\n",
    "        else:\n",
    "            v.append(count)\n",
    "            count = 1\n",
    "            i +=1\n",
    "    v.append(count)        \n",
    "    return v\n",
    "\n",
    "def plotPrincipalComponents(X, dim):\n",
    "    v = getClassNumber(y_train)\n",
    "    colors = 'b', 'g', 'r', 'c', 'm', 'y', 'k', 'grey', 'orange', 'purple'\n",
    "    markers = ['o', 'x' , 'v', 'd']\n",
    "    tot = len(X)\n",
    "    start = 0 \n",
    "    if(dim == 2):\n",
    "        for i,index in enumerate(v):\n",
    "            end = start + index\n",
    "            plt.scatter(X[start:end,0],X[start:end,1] , color=colors[i%len(colors)], marker=markers[i%len(markers)], label = fruits[i])\n",
    "            start = end\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "    \n",
    "    if(dim == 3):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        for i,index in enumerate(v):\n",
    "            end = start + index\n",
    "            ax.scatter(X[start:end,0], X[start:end,1], X[start:end,2], color=colors[i%len(colors)], marker=markers[i%len(markers)], label = fruits[i])\n",
    "            start = end\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.set_zlabel('PC3')\n",
    "\n",
    "\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.xticks()\n",
    "    plt.yticks()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=fruits, yticklabels=fruits,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return cm,ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to discover how our data appears in lower dimension we need to reduce dimensionality of the dataset in 2 or 3 dimension so that we can plot and visualize them. To do this I've decided to use Principal Component Analis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA IN 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "dataIn2D = pca.fit_transform(X_train)\n",
    "plotPrincipalComponents(dataIn2D, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA IN 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "dataIn3D = pca.fit_transform(X_train)\n",
    "plotPrincipalComponents(dataIn3D, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def showPCA(image,X2, X10, X50):\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    ax1 = fig.add_subplot(1,4,1)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Original image')\n",
    "    plt.imshow(image)\n",
    "    ax1 = fig.add_subplot(1,4,2)\n",
    "    ax1.axis('off') \n",
    "    ax1.set_title('50 PC')\n",
    "    plt.imshow(X50)\n",
    "    ax1 = fig.add_subplot(1,4,3)\n",
    "    ax1.axis('off') \n",
    "    ax1.set_title('10 PC')\n",
    "    plt.imshow(X10)\n",
    "    ax2 = fig.add_subplot(1,4,4)\n",
    "    ax2.axis('off') \n",
    "    ax2.set_title('2 PC')\n",
    "    plt.imshow(X2)\n",
    "    plt.show()\n",
    "\n",
    "def computePCA(n, im_scaled, image_id):\n",
    "    pca = PCA(n)\n",
    "    principalComponents = pca.fit_transform(im_scaled)\n",
    "    im_reduced = pca.inverse_transform(principalComponents)\n",
    "    newImage = scaler.inverse_transform(im_reduced[image_id])\n",
    "    return newImage\n",
    "\n",
    "def showVariance(X_train):\n",
    "    #Compute manually the principal components\n",
    "    cov_matr=np.dot(X_train, X_train.T)\n",
    "    eigval,eigvect=np.linalg.eig(cov_matr)\n",
    "\n",
    "    index=np.argsort(eigval)[::-1] #take in order the index of ordered vector (ascending order)\n",
    "\n",
    "    #eigvect[:,i] is associated to eigval[i] so \n",
    "    eigvect=eigvect[:,index]\n",
    "    eigval=eigval[index]\n",
    "\n",
    "    n_PC=[]\n",
    "    var_explained=[]\n",
    "    var_temp=[]\n",
    "    var_tmp=0\n",
    "    for i in range(10):\n",
    "        var_tmp=var_tmp+eigval[i]\n",
    "        n_PC.append(i)\n",
    "        var_temp.append(eigval[i]/(eigval.sum())*100)\n",
    "        var_explained.append(var_tmp/(eigval.sum())*100)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "    ind = np.arange(10)    \n",
    "    width = 0.35         # the width of the bars\n",
    "    p1 = ax.bar(ind, var_temp, width, color='b')\n",
    "    p2 = ax.bar(ind + width, var_explained, width, color='r')\n",
    "\n",
    "    ax.legend((p1[0], p2[0]), ('Individual explained variance', 'Cumulative explained variance'))\n",
    "\n",
    "    ax.set_title('Variance explained using PCs')\n",
    "    ax.set_xticks(ind + width / 2)\n",
    "    ax.set_xticklabels(('1', '2', '3', '4', '5', '6', '7', '8', '9', '10'))\n",
    "\n",
    "    plt.xlabel('Number of PC')\n",
    "    plt.ylabel('Variance exaplained in %')\n",
    "\n",
    "    ax.autoscale_view()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = 2\n",
    "image = X_t[image_id]\n",
    "\n",
    "#Compute PCA\n",
    "X_2 = computePCA(2, X_train,image_id)\n",
    "X_10 = computePCA(10, X_train,image_id)\n",
    "X_50 = computePCA(50, X_train,image_id)\n",
    "\n",
    "#Reshape in order to plot images\n",
    "X2 = np.reshape(X_2, (dim,dim,3)).astype(int)\n",
    "X10 = np.reshape(X_10, (dim,dim,3)).astype(int)\n",
    "X50 = np.reshape(X_50, (dim,dim,3)).astype(int)\n",
    "\n",
    "#Plot\n",
    "showPCA(image, X2, X10, X50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously for a classification algorithm accuracy of the classification will be lower but instead training time will be faster, if the classes were linearly separable the accuracy could be satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VARIANCE EXPLAINED USING PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showVariance(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMMENT\n",
    "\n",
    "As we can see from this graph the first PC captures only 16-17% of variance, using first two the percentage of information captured is less than **30%**. \n",
    "\n",
    "In the next section I'll perform also classification using only first two dimension, this is not to get a good accuracy, because with so low information classifiers will not be able to learn so much, but instead is made in order to show **decision boundaries** of classifiers when using two dimensions. \n",
    "\n",
    "\n",
    "Using first 10PC, variance captured is **40%**, not a bad results considering that the dataset has hundreds of dimension.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BINARY CLASSIFICATION\n",
    "Let's start now to classify our dataset, firstly we take only two class in order to perform a classic binary classification, at the end the entire dataset (or a sub-part of it) will be classified. \n",
    "I'll use 3 different technique: **SVM**, **K-NN**, **Decision Tree**.<br>\n",
    "At the end of the Binary Classification there will be a comparison between all the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LINEAR SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(gamma='auto', kernel='linear', probability=True)\n",
    "svm.fit(X_train, y_train) \n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "#Evaluation\n",
    "precision = metrics.accuracy_score(y_pred, y_test) * 100\n",
    "print(\"Accuracy with SVM: {0:.2f}%\".format(precision))\n",
    "cm , _ = plot_confusion_matrix(y_test, y_pred,classes=y_train, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# calculate the FPR and TPR for all thresholds of the classification\n",
    "probs = svm.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "svm_fpr, svm_tpr, thresholds = metrics.roc_curve(y_test, probs)\n",
    "svm_auc = metrics.roc_auc_score(y_test, probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM + K-FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kfold = cross_val_score(svm, X, y, cv=5) \n",
    "print(\"Accuracy with SVM and K-FOLD CROSS VALIDATION: %0.2f (+/- %0.2f)\" % (pred_kfold.mean(), pred_kfold.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LINEAR SVM + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train2D = pca.fit_transform(X_train)\n",
    "X_test2D = pca.fit_transform(X_test)\n",
    "\n",
    "svm.fit(X_train2D, y_train) \n",
    "test_predictions = svm.predict(X_test2D)\n",
    "precision = metrics.accuracy_score(test_predictions, y_test) * 100\n",
    "print(\"Accuracy with SVM considering only first 2PC: {0:.2f}%\".format(precision))\n",
    "\n",
    "#Plotting decision boundaries\n",
    "plot_decision_regions(X_train2D, y_train, clf=svm, legend=1)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Linear SVM Decision Boundaries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KERNEL SVM + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_with_kernel = SVC(gamma=0.01, kernel='rbf', probability=True)\n",
    "svm_with_kernel.fit(X_train2D, y_train) \n",
    "y_pred = svm_with_kernel.predict(X_test2D)\n",
    "precision = metrics.accuracy_score(y_pred, y_test) * 100\n",
    "print(\"Accuracy with Not-Linear SVM considering only first 2PC: {0:.2f}%\".format(precision))\n",
    "\n",
    "#Plotting decision boundaries\n",
    "plot_decision_regions(X_train2D, y_train, clf=svm_with_kernel, legend=1)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Kernel SVM Decision Boundaries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "#Evaluation\n",
    "precision = metrics.accuracy_score(y_pred, y_test) * 100\n",
    "print(\"Accuracy with K-NN: {0:.2f}%\".format(precision))\n",
    "cm , _ = plot_confusion_matrix(y_test, y_pred, classes=y_train, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# calculate the FPR and TPR for all thresholds of the classification\n",
    "probs = knn.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "knn_fpr, knn_tpr, thresholds = metrics.roc_curve(y_test, probs)\n",
    "knn_auc = metrics.roc_auc_score(y_test, probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN + K-FOLD\n",
    "pred_kfold = cross_val_score(knn, X, y, cv=5) \n",
    "print(\"Accuracy with K-NN and K-FOLD CROSS VALIDATION: %0.2f (+/- %0.2f)\" % (pred_kfold.mean(), pred_kfold.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#CHANGING VALUES OF N\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "\n",
    "for i in range(1,15):   #check all possible values for 1 to 15\n",
    "    k_nn = KNeighborsClassifier(n_neighbors=i)\n",
    "    k_nn.fit(X_train,y_train)\n",
    "    pred_i = k_nn.predict(X_test)\n",
    "    accuracy_train.append(k_nn.score(X_train,y_train)*100)\n",
    "    accuracy_test.append(k_nn.score(X_test,y_test)*100)\n",
    "    \n",
    "accuracy_train_array=np.asarray(accuracy_train)\n",
    "accuracy_test_array=np.asarray(accuracy_test)\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,15),accuracy_train_array, label='Training_Accuracy', color='blue')\n",
    "plt.plot(range(1,15),accuracy_test_array, label='Testing_Accuracy', color='red')\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs K value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy%')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-NN + PCA\n",
    "knn.fit(X_train2D, y_train)\n",
    "y_pred = knn.predict(X_test2D)\n",
    "precision = metrics.accuracy_score(y_pred, y_test) * 100\n",
    "print(\"Accuracy with K-NN considering only first 2PC: {0:.2f}%\".format(precision))\n",
    "\n",
    "#Plotting decision boundaries\n",
    "plot_decision_regions(X_train2D, y_train, clf=knn, legend=1)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('K-NN Decision Boundaries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "tree = tree.fit(X_train,y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "#Evaluation\n",
    "precision = metrics.accuracy_score(y_pred, y_test) * 100\n",
    "print(\"Accuracy with Decision Tree: {0:.2f}%\".format(precision))\n",
    "cm , _ = plot_confusion_matrix(y_test, y_pred, classes=y_train, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()\n",
    "\n",
    "# calculate the FPR and TPR for all thresholds of the classification\n",
    "probs = tree.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "tree_fpr, tree_tpr, thresholds = metrics.roc_curve(y_test, probs)\n",
    "tree_auc = metrics.roc_auc_score(y_test, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECISION TREE + K-FOLD\n",
    "pred_kfold = cross_val_score(tree, X, y, cv=5) \n",
    "print(\"Accuracy with DECISION TREE and K-FOLD CROSS VALIDATION: %0.2f (+/- %0.2f)\" % (pred_kfold.mean(), pred_kfold.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# CHANGING MAX_DEPTH\n",
    "score_train=[]\n",
    "score_test=[]\n",
    "\n",
    "for i in range(1,10):\n",
    "    dtree_md = DecisionTreeClassifier(max_depth=i)\n",
    "    dtree_md.fit(X_train,y_train)\n",
    "    \n",
    "    score_train.append(dtree_md.score(X_train,y_train)*100)\n",
    "    score_test.append(dtree_md.score(X_test,y_test)*100)\n",
    "    \n",
    "score_train_array=np.asarray(score_train)\n",
    "score_test_array=np.asarray(score_test)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,10),score_train_array,color='blue', label=\"Training_accuracy\")\n",
    "plt.plot(range(1,10),score_test_array,color='red',label=\"Testing_accuracy\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Accuracy%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on Training phase increase while the Accuracy on Test phase is decreasing, this means that the model **overfit** increasing the max depth of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECISION TREE + PCA\n",
    "tree = tree.fit(X_train2D,y_train)\n",
    "y_pred = tree.predict(X_test2D)\n",
    "precision = metrics.accuracy_score(y_pred, y_test) * 100\n",
    "print(\"Accuracy with Decision Tree considering only first 2PC: {0:.2f}%\".format(precision))\n",
    "\n",
    "#Plotting decision boundaries\n",
    "plot_decision_regions(X_train2D, y_train, clf=tree, legend=1)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Decision Tree Decision Boundaries')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#ROC CURVE\n",
    "plt.title('ROC Curve')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(svm_fpr, svm_tpr, 'b', marker='.', label = 'SVM = %0.3f' % svm_auc )\n",
    "plt.plot(knn_fpr, knn_tpr, 'g', marker='.', label = 'K-NN = %0.3f' % knn_auc)\n",
    "plt.plot(tree_fpr, tree_tpr, 'r', marker='.',label = 'DECISION TREE = %.3f' % tree_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMMENT:\n",
    "SVM and K-NN performs better with this classification with an AUC = 0.99x, instead Decision Tree is worst with an AUC of 0.65 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-CLASS CLASSIFICATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = ['Orange', 'Banana' , 'Strawberry', 'Apple Golden 1', 'Kiwi' , 'Lemon', 'Cocos' , 'Pineapple' , 'Peach', 'Cherry 1', 'Cherry 2', 'Mandarine']\n",
    "#fruits = getAllFruits() #Be sure to have enough free memory\n",
    "\n",
    "#Get Images and Labels\n",
    "X, y =  getYourFruits(fruits, 'Training')\n",
    "X_test, y_test = getYourFruits(fruits, 'Test')\n",
    "\n",
    "#Scale Data Images\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform([i.flatten() for i in X])\n",
    "X_test = scaler.fit_transform([i.flatten() for i in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "model = SVC(gamma='auto', kernel='linear')\n",
    "model.fit(X_train, y) \n",
    "y_pred = model.predict(X_test)\n",
    "precision = metrics.accuracy_score(y_pred, y_test) * 100\n",
    "print(\"Accuracy with SVM: {0:.2f}%\".format(precision))\n",
    "\n",
    "#K-NN\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y)\n",
    "y_pred = model.predict(X_test)\n",
    "precision = metrics.accuracy_score(y_pred, y_test) * 100\n",
    "print(\"Accuracy with K-NN: {0:.2f}%\".format(precision))\n",
    "\n",
    "#DECISION TREE\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train,y)\n",
    "y_pred = model.predict(X_test)\n",
    "precision = metrics.accuracy_score(y_pred, y_test) * 100\n",
    "print(\"Accuracy with Decision Tree: {0:.2f}%\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMMENT \n",
    "SVM is the classification algorithm that performs better in the multi-class classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
